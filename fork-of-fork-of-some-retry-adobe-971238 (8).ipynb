{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9526829,"sourceType":"datasetVersion","datasetId":5801331},{"sourceId":9548807,"sourceType":"datasetVersion","datasetId":5817858}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:26:55.024227Z","iopub.execute_input":"2024-10-12T15:26:55.024539Z","iopub.status.idle":"2024-10-12T15:26:55.029066Z","shell.execute_reply.started":"2024-10-12T15:26:55.024506Z","shell.execute_reply":"2024-10-12T15:26:55.028042Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_path = '/kaggle/input/adobetraindata/behaviour_simulation_train.csv'\ntest_path = '/kaggle/input/inter-iit-mid-prep-adobe/problem_1_test_dataset/behaviour_simulation_test_company.xlsx'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:26:55.030470Z","iopub.execute_input":"2024-10-12T15:26:55.030843Z","iopub.status.idle":"2024-10-12T15:26:55.040792Z","shell.execute_reply.started":"2024-10-12T15:26:55.030810Z","shell.execute_reply":"2024-10-12T15:26:55.039887Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"test_dataset = pd.read_excel(test_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:26:55.041855Z","iopub.execute_input":"2024-10-12T15:26:55.042196Z","iopub.status.idle":"2024-10-12T15:26:57.293880Z","shell.execute_reply.started":"2024-10-12T15:26:55.042160Z","shell.execute_reply":"2024-10-12T15:26:57.292642Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"test_dataset.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:26:57.295509Z","iopub.execute_input":"2024-10-12T15:26:57.295888Z","iopub.status.idle":"2024-10-12T15:26:57.313610Z","shell.execute_reply.started":"2024-10-12T15:26:57.295849Z","shell.execute_reply":"2024-10-12T15:26:57.312446Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 6 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   id                10000 non-null  int64 \n 1   date              10000 non-null  object\n 2   content           10000 non-null  object\n 3   username          10000 non-null  object\n 4   media             10000 non-null  object\n 5   inferred company  10000 non-null  object\ndtypes: int64(1), object(5)\nmemory usage: 468.9+ KB\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"train_dataset = pd.read_csv(train_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:32:17.425795Z","iopub.execute_input":"2024-10-12T15:32:17.426499Z","iopub.status.idle":"2024-10-12T15:32:20.611402Z","shell.execute_reply.started":"2024-10-12T15:32:17.426455Z","shell.execute_reply":"2024-10-12T15:32:20.610395Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:32:20.613607Z","iopub.execute_input":"2024-10-12T15:32:20.614070Z","iopub.status.idle":"2024-10-12T15:32:20.631691Z","shell.execute_reply.started":"2024-10-12T15:32:20.614020Z","shell.execute_reply":"2024-10-12T15:32:20.630502Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"            id                 date  likes  \\\n0            1  2020-12-12 00:47:00      1   \n1            2  2018-06-30 10:04:20   2750   \n2            3  2020-09-29 19:47:28     57   \n3            4  2020-10-01 11:40:09    152   \n4            5  2018-10-19 14:30:46     41   \n...        ...                  ...    ...   \n299995  299996  2019-09-07 16:18:10      0   \n299996  299997  2018-02-23 11:24:36     46   \n299997  299998  2020-11-11 20:18:15    261   \n299998  299999  2019-10-29 10:44:00    119   \n299999  300000  2018-08-26 01:19:09    714   \n\n                                                  content        username  \\\n0       Spend your weekend morning with a Ham, Egg, an...    TimHortonsPH   \n1       Watch rapper <mention> freestyle for over an H...       IndyMusic   \n2       Canadian Armenian community demands ban on mil...       CBCCanada   \n3       1st in Europe to be devastated by COVID-19, It...  MKWilliamsRome   \n4       Congratulations to Pauletha Butts of <mention>...           BGISD   \n...                                                   ...             ...   \n299995  Barcelona Star Expected To Return Against Vale...  IndependentNGR   \n299996  Kjeld Nuis of #NED is golden again... This tim...     CBCOlympics   \n299997  Grateful üôåüèæ to have <mention> officially join ...   TeamCoachBuzz   \n299998  The US Air Force's mysterious space plane just...     Independent   \n299999  In 1967, he was shot down over Vietnam. By 200...            cnni   \n\n                                                    media inferred company  \n0       [Photo(previewUrl='https://pbs.twimg.com/media...      tim hortons  \n1       [Photo(previewUrl='https://pbs.twimg.com/media...      independent  \n2       [Photo(previewUrl='https://pbs.twimg.com/media...              cbc  \n3       [Photo(previewUrl='https://pbs.twimg.com/media...         williams  \n4       [Photo(previewUrl='https://pbs.twimg.com/media...      independent  \n...                                                   ...              ...  \n299995  [Photo(previewUrl='https://pbs.twimg.com/media...      independent  \n299996  [Video(thumbnailUrl='https://pbs.twimg.com/med...              cbc  \n299997  [Photo(previewUrl='https://pbs.twimg.com/media...         williams  \n299998  [Video(thumbnailUrl='https://pbs.twimg.com/med...      independent  \n299999  [Video(thumbnailUrl='https://pbs.twimg.com/ext...              cnn  \n\n[300000 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>likes</th>\n      <th>content</th>\n      <th>username</th>\n      <th>media</th>\n      <th>inferred company</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2020-12-12 00:47:00</td>\n      <td>1</td>\n      <td>Spend your weekend morning with a Ham, Egg, an...</td>\n      <td>TimHortonsPH</td>\n      <td>[Photo(previewUrl='https://pbs.twimg.com/media...</td>\n      <td>tim hortons</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2018-06-30 10:04:20</td>\n      <td>2750</td>\n      <td>Watch rapper &lt;mention&gt; freestyle for over an H...</td>\n      <td>IndyMusic</td>\n      <td>[Photo(previewUrl='https://pbs.twimg.com/media...</td>\n      <td>independent</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2020-09-29 19:47:28</td>\n      <td>57</td>\n      <td>Canadian Armenian community demands ban on mil...</td>\n      <td>CBCCanada</td>\n      <td>[Photo(previewUrl='https://pbs.twimg.com/media...</td>\n      <td>cbc</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2020-10-01 11:40:09</td>\n      <td>152</td>\n      <td>1st in Europe to be devastated by COVID-19, It...</td>\n      <td>MKWilliamsRome</td>\n      <td>[Photo(previewUrl='https://pbs.twimg.com/media...</td>\n      <td>williams</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2018-10-19 14:30:46</td>\n      <td>41</td>\n      <td>Congratulations to Pauletha Butts of &lt;mention&gt;...</td>\n      <td>BGISD</td>\n      <td>[Photo(previewUrl='https://pbs.twimg.com/media...</td>\n      <td>independent</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>299995</th>\n      <td>299996</td>\n      <td>2019-09-07 16:18:10</td>\n      <td>0</td>\n      <td>Barcelona Star Expected To Return Against Vale...</td>\n      <td>IndependentNGR</td>\n      <td>[Photo(previewUrl='https://pbs.twimg.com/media...</td>\n      <td>independent</td>\n    </tr>\n    <tr>\n      <th>299996</th>\n      <td>299997</td>\n      <td>2018-02-23 11:24:36</td>\n      <td>46</td>\n      <td>Kjeld Nuis of #NED is golden again... This tim...</td>\n      <td>CBCOlympics</td>\n      <td>[Video(thumbnailUrl='https://pbs.twimg.com/med...</td>\n      <td>cbc</td>\n    </tr>\n    <tr>\n      <th>299997</th>\n      <td>299998</td>\n      <td>2020-11-11 20:18:15</td>\n      <td>261</td>\n      <td>Grateful üôåüèæ to have &lt;mention&gt; officially join ...</td>\n      <td>TeamCoachBuzz</td>\n      <td>[Photo(previewUrl='https://pbs.twimg.com/media...</td>\n      <td>williams</td>\n    </tr>\n    <tr>\n      <th>299998</th>\n      <td>299999</td>\n      <td>2019-10-29 10:44:00</td>\n      <td>119</td>\n      <td>The US Air Force's mysterious space plane just...</td>\n      <td>Independent</td>\n      <td>[Video(thumbnailUrl='https://pbs.twimg.com/med...</td>\n      <td>independent</td>\n    </tr>\n    <tr>\n      <th>299999</th>\n      <td>300000</td>\n      <td>2018-08-26 01:19:09</td>\n      <td>714</td>\n      <td>In 1967, he was shot down over Vietnam. By 200...</td>\n      <td>cnni</td>\n      <td>[Video(thumbnailUrl='https://pbs.twimg.com/ext...</td>\n      <td>cnn</td>\n    </tr>\n  </tbody>\n</table>\n<p>300000 rows √ó 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport xgboost as xgb\n# Load data\ntrain_data = train_dataset.copy()\ntest_data = test_dataset.copy()\n# Bin the likes into categories\n# Define the cap threshold\ncap_value = 1000000\n# Apply the cap to the 'likes' column\ntrain_data['likes_capped'] = train_data['likes'].apply(lambda x: min(x, cap_value))\n# Check the distribution of the capped 'likes' column\nprint(train_data['likes_capped'].describe())\nbins = [0, 100, 1000, 10000, cap_value]\nlabels = ['0-100', '101-1k','1k-10k','10k+']\ntrain_data['likes_binned'] = pd.cut(train_data['likes_capped'], bins=bins, labels=labels, include_lowest=True)\n# Sample 10% of the data, stratified by binned likes\nsample_fraction = 0.01  # Change this to 1.0 for 100%\ntrain_data_sampled, _ = train_test_split(train_data, test_size=1-sample_fraction, random_state=42, stratify=train_data['likes_binned'])\ntrain_data_sampled = train_data_sampled.reset_index(drop=1)\n# train_data_sampled = train_data.copy()\n\n# Check distribution\nprint(train_data_sampled['likes_binned'].value_counts(normalize=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:32:20.633043Z","iopub.execute_input":"2024-10-12T15:32:20.633425Z","iopub.status.idle":"2024-10-12T15:32:21.617545Z","shell.execute_reply.started":"2024-10-12T15:32:20.633384Z","shell.execute_reply":"2024-10-12T15:32:21.616378Z"}},"outputs":[{"name":"stdout","text":"count    300000.000000\nmean        773.364793\nstd        4931.463419\nmin           0.000000\n25%           3.000000\n50%          76.000000\n75%         364.000000\nmax      560193.000000\nName: likes_capped, dtype: float64\nlikes_binned\n0-100     0.548667\n101-1k    0.327333\n1k-10k    0.111667\n10k+      0.012333\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"import re\ndef remove_company_name(username, companies=train_data_sampled['inferred company']):\n    \"\"\"\n    Remove company name from a username.\n    \n    Args:\n    username (str): The username to process.\n    companies (list): A list of company names to look for.\n    \n    Returns:\n    str: The username with the company name removed, if found.\n    \"\"\"\n    compnaies = list(set(companies))\n    # Sort companies by length (longest first) to avoid partial matches\n    sorted_companies = sorted(companies, key=len, reverse=True)\n    \n    # Convert username to lowercase for case-insensitive matching\n    lowercase_username = username.lower()\n    \n    for company in sorted_companies:\n        # Convert company to lowercase and remove spaces\n        company_pattern = company.lower().replace(' ', '')\n        \n        # Check if the company name is in the username\n        if company_pattern in lowercase_username:\n            # Remove the company name (case-insensitive)\n            return re.sub(re.escape(company_pattern), '', username, flags=re.IGNORECASE)\n    \n    # If no company name was found, return the original username\n    return username","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:32:21.619941Z","iopub.execute_input":"2024-10-12T15:32:21.620344Z","iopub.status.idle":"2024-10-12T15:32:21.629019Z","shell.execute_reply.started":"2024-10-12T15:32:21.620282Z","shell.execute_reply":"2024-10-12T15:32:21.627954Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"train_data_sampled['username_cleaned'] = train_data_sampled['username'].apply(remove_company_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:32:21.630189Z","iopub.execute_input":"2024-10-12T15:32:21.630588Z","iopub.status.idle":"2024-10-12T15:32:26.925374Z","shell.execute_reply.started":"2024-10-12T15:32:21.630551Z","shell.execute_reply":"2024-10-12T15:32:26.924446Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"try:\n    train_data_sampled['old_username'],train_data_sampled['username'] = train_data_sampled['old_username'],train_data_sampled['username_cleaned']\nexcept:\n    train_data_sampled['old_username'],train_data_sampled['username'] = train_data_sampled['username'],train_data_sampled['username_cleaned']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:32:26.926622Z","iopub.execute_input":"2024-10-12T15:32:26.926972Z","iopub.status.idle":"2024-10-12T15:32:26.933662Z","shell.execute_reply.started":"2024-10-12T15:32:26.926935Z","shell.execute_reply":"2024-10-12T15:32:26.932555Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"!pip install bertopic tweet-preprocessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:33:37.442779Z","iopub.execute_input":"2024-10-12T15:33:37.443803Z","iopub.status.idle":"2024-10-12T15:33:52.570254Z","shell.execute_reply.started":"2024-10-12T15:33:37.443746Z","shell.execute_reply":"2024-10-12T15:33:52.568875Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bertopic\n  Downloading bertopic-0.16.4-py3-none-any.whl.metadata (23 kB)\nCollecting tweet-preprocessor\n  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl.metadata (5.9 kB)\nCollecting hdbscan>=0.8.29 (from bertopic)\n  Downloading hdbscan-0.8.39-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from bertopic) (1.26.4)\nRequirement already satisfied: pandas>=1.1.5 in /opt/conda/lib/python3.10/site-packages (from bertopic) (2.2.2)\nRequirement already satisfied: plotly>=4.7.0 in /opt/conda/lib/python3.10/site-packages (from bertopic) (5.22.0)\nRequirement already satisfied: scikit-learn>=0.22.2.post1 in /opt/conda/lib/python3.10/site-packages (from bertopic) (1.2.2)\nCollecting sentence-transformers>=0.4.1 (from bertopic)\n  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: tqdm>=4.41.1 in /opt/conda/lib/python3.10/site-packages (from bertopic) (4.66.4)\nCollecting umap-learn>=0.5.0 (from bertopic)\n  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\nRequirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2024.1)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic) (8.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic) (21.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.45.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (10.3.0)\nRequirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\nCollecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.6.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.12.2)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->plotly>=4.7.0->bertopic) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\nDownloading bertopic-0.16.4-py3-none-any.whl (143 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\nDownloading hdbscan-0.8.39-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tweet-preprocessor, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\nSuccessfully installed bertopic-0.16.4 hdbscan-0.8.39 pynndescent-0.5.13 sentence-transformers-3.2.0 tweet-preprocessor-0.6.0 umap-learn-0.5.6\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"!pip install node2vec","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-10-12T15:27:06.350160Z","iopub.execute_input":"2024-10-12T15:27:06.350863Z","iopub.status.idle":"2024-10-12T15:27:28.375696Z","shell.execute_reply.started":"2024-10-12T15:27:06.350810Z","shell.execute_reply":"2024-10-12T15:27:28.374572Z"}},"outputs":[{"name":"stdout","text":"Collecting node2vec\n  Downloading node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\nRequirement already satisfied: gensim<5.0.0,>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from node2vec) (4.3.3)\nRequirement already satisfied: joblib<2.0.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from node2vec) (1.4.2)\nRequirement already satisfied: networkx<4.0.0,>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from node2vec) (3.3)\nRequirement already satisfied: numpy<2.0.0,>=1.24.0 in /opt/conda/lib/python3.10/site-packages (from node2vec) (1.26.4)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from node2vec) (4.66.4)\nCollecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.0->node2vec)\n  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.0.4)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (1.16.0)\nDownloading node2vec-0.5.0-py3-none-any.whl (7.2 kB)\nDownloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy, node2vec\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.14.1\n    Uninstalling scipy-1.14.1:\n      Successfully uninstalled scipy-1.14.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed node2vec-0.5.0 scipy-1.13.1\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.preprocessing import OneHotEncoder\nfrom datetime import datetime\nimport networkx as nx\nfrom node2vec import Node2Vec\nimport re\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nimport emoji\n\n# Ensure you've downloaded necessary NLTK data\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-10-12T15:27:28.377567Z","iopub.execute_input":"2024-10-12T15:27:28.378024Z","iopub.status.idle":"2024-10-12T15:27:49.635541Z","shell.execute_reply.started":"2024-10-12T15:27:28.377977Z","shell.execute_reply":"2024-10-12T15:27:49.634369Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# def preprocess_text(text):\n#     # Convert to lowercase\n#     text = text.lower()\n#     # Remove URLs\n#     text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n#     # Remove user mentions\n#     text = re.sub(r'@\\w+', '', text)\n#     # Remove non-letter characters\n#     text = re.sub(r'[^a-zA-Z\\s]', '', text)\n#     # Remove extra spaces\n#     text = re.sub(r'\\s+', ' ', text).strip()\n#     # Remove stopwords\n#     stop_words = set(stopwords.words('english'))\n#     text = ' '.join([word for word in text.split() if word not in stop_words])\n#     return text\n\n# def get_roberta_embedding(text):\n#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n#     with torch.no_grad():\n#         outputs = model(**inputs)\n#     return outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n\n# def get_datetime_features(date_str):\n#     date = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n#     return np.array([\n#         date.year, date.month, date.day, date.hour, date.minute,\n#         date.weekday(), date.timetuple().tm_yday,\n#         np.sin(2 * np.pi * date.hour / 24),  # Cyclical encoding for hour\n#         np.cos(2 * np.pi * date.hour / 24),\n#         np.sin(2 * np.pi * date.month / 12),  # Cyclical encoding for month\n#         np.cos(2 * np.pi * date.month / 12)\n#     ])\n\n# def get_categorical_embedding(value, encoder):\n#     return encoder.transform([[value]]).toarray().flatten()\n\n# def create_graph_embedding(df):\n#     G = nx.Graph()\n#     for _, row in df.iterrows():\n#         G.add_edge(row['username'], row['inferred company'])\n#         for mention in re.findall(r'@(\\w+)', row['content']):\n#             G.add_edge(row['username'], mention)\n    \n#     node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n#     model = node2vec.fit(window=10, min_count=1, batch_words=4)\n    \n#     def get_node_embedding(node):\n#         try:\n#             return model.wv[node]\n#         except KeyError:\n#             return np.zeros(64)\n    \n#     df['user_graph_embedding'] = df['username'].apply(get_node_embedding)\n#     df['company_graph_embedding'] = df['inferred company'].apply(get_node_embedding)\n#     return df\n\n# def extract_hashtags(text):\n#     return ' '.join(re.findall(r'#(\\w+)', text))\n\n# def extract_emoji(text):\n#     return ' '.join([c for c in text if c in emoji.UNICODE_EMOJI['en']])\n\n# # Load the data\n# df = pd.read_csv(\"tweets.csv\")\n\n# # Preprocess text\n# df['processed_content'] = df['content'].apply(preprocess_text)\n# df['hashtags'] = df['content'].apply(extract_hashtags)\n# df['emojis'] = df['content'].apply(extract_emoji)\n\n# # Combine all text data\n# df['combined_text'] = df.apply(lambda row: f\"{row['processed_content']} {row['username']} {row['inferred company']} {row['hashtags']} {row['emojis']}\", axis=1)\n\n# # Generate text embeddings\n# df['combined_embedding'] = df['combined_text'].apply(get_roberta_embedding)\n\n# # Generate datetime embeddings\n# df['datetime_embedding'] = df['date'].apply(get_datetime_features)\n\n# # Generate categorical embeddings\n# company_encoder = OneHotEncoder(sparse=False)\n# company_encoder.fit(df['inferred company'].values.reshape(-1, 1))\n# df['company_embedding'] = df['inferred company'].apply(lambda x: get_categorical_embedding(x, company_encoder))\n\n# # Generate graph embeddings\n# df = create_graph_embedding(df)\n\n# # Combine all embeddings\n# df['rich_embedding'] = df.apply(lambda row: np.concatenate([\n#     row['combined_embedding'],\n#     row['datetime_embedding'],\n#     row['company_embedding'],\n#     row['user_graph_embedding'],\n#     row['company_graph_embedding']\n# ]), axis=1)\n\n# print(\"Rich embeddings generated successfully!\")\n# print(f\"Shape of rich embedding: {df['rich_embedding'].iloc[0].shape}\")\n\n# # Optional: Dimensionality reduction if needed\n# # from sklearn.decomposition import PCA\n# # pca = PCA(n_components=100)\n# # df['reduced_embedding'] = list(pca.fit_transform(np.stack(df['rich_embedding'])))\n\n# # Save the embeddings\n# np.save('rich_tweet_embeddings.npy', np.stack(df['rich_embedding']))\n# df[['id', 'content', 'username', 'inferred company']].to_csv('tweet_metadata.csv', index=False)\n\n# print(\"Embeddings and metadata saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:27:49.637467Z","iopub.execute_input":"2024-10-12T15:27:49.638383Z","iopub.status.idle":"2024-10-12T15:27:49.647533Z","shell.execute_reply.started":"2024-10-12T15:27:49.638328Z","shell.execute_reply":"2024-10-12T15:27:49.646368Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# tokenize_username('AAASouthJersey')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:27:49.648970Z","iopub.execute_input":"2024-10-12T15:27:49.649467Z","iopub.status.idle":"2024-10-12T15:27:49.663032Z","shell.execute_reply.started":"2024-10-12T15:27:49.649409Z","shell.execute_reply":"2024-10-12T15:27:49.662002Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# set(df['username'])","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-10-12T15:27:49.664355Z","iopub.execute_input":"2024-10-12T15:27:49.664762Z","iopub.status.idle":"2024-10-12T15:27:49.673999Z","shell.execute_reply.started":"2024-10-12T15:27:49.664717Z","shell.execute_reply":"2024-10-12T15:27:49.673033Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"set(train_data_sampled['username'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:27:49.675514Z","iopub.execute_input":"2024-10-12T15:27:49.676229Z","iopub.status.idle":"2024-10-12T15:27:49.699589Z","shell.execute_reply.started":"2024-10-12T15:27:49.676170Z","shell.execute_reply":"2024-10-12T15:27:49.698544Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'',\n '04_en',\n '112',\n '365',\n '9A',\n 'A4advisors',\n 'ALKCM',\n 'AMESA',\n 'ANZ',\n 'ARTravs',\n 'ASPolice',\n 'AU',\n 'Aaryn_',\n 'Abbie44',\n 'Actually',\n 'Ads',\n 'Africa',\n 'AhmadJawadBTH',\n 'Airlines',\n 'Airplanes',\n 'Alerts',\n 'AlexArthur1',\n 'Allan247',\n 'Amer',\n 'America',\n 'AmyMBE',\n 'Andy_',\n 'Anomia',\n 'Arabia',\n 'ArlanWasHere',\n 'Arthurvw1986',\n 'ArtisanFilms',\n 'Arts',\n 'Azure',\n 'BGISD',\n 'BWWings',\n 'BWWingsOM',\n 'Barrett',\n 'Beacon',\n 'BlackCom',\n 'Boomin',\n 'BornFDN',\n 'BornUSA',\n 'Brando',\n 'Brett_',\n 'Business',\n 'ByTMobile',\n 'CAGaming',\n 'CJCJ_RL',\n 'CRacing',\n 'CSChallengeFW',\n 'CX',\n 'CaJBoyce',\n 'Calgary',\n 'Cam12Delport',\n 'Cam_Cawthorne',\n 'Canada',\n 'CaseyTheVA',\n 'Center',\n 'Champion',\n 'ChantalOnAir',\n 'ChariTs',\n 'CharlesPulliam',\n 'CheetahsRugby',\n 'ChionsCup',\n 'ChristelFox4KC',\n 'Cloud',\n 'ClubAlliance',\n 'CoachShawn',\n 'Collab',\n 'College',\n 'ComRes',\n 'Community',\n 'Costellop',\n 'Cricketnation',\n 'Crowe',\n 'Ctr',\n 'DC',\n 'Data',\n 'DavidG',\n 'DePaulU',\n 'Defense',\n 'DevNet',\n 'Dove',\n 'DrColvin',\n 'DrLisaMP',\n 'Drive',\n 'DubesAustralia',\n 'E15',\n 'EDU',\n 'ELehmannTV',\n 'EMR_Automation',\n 'EUPolicy',\n 'Edmonton',\n 'EduCA',\n 'EmCollective',\n 'Energy',\n 'EnergyME',\n 'Esports',\n 'EuroLeague',\n 'Europe',\n 'Events',\n 'FMSA',\n 'FaZe_Kitty',\n 'FanZone',\n 'Financial',\n 'FireRescue',\n 'FoodWarsAnime',\n 'FootySuperTips',\n 'Found',\n 'GR',\n 'Gamesa',\n 'Gaming',\n 'GaryGannTD',\n 'Gavin',\n 'GettySport',\n 'GettyVIP',\n 'Ghana',\n 'Golf',\n 'Greedy',\n 'GuideUK',\n 'HMcEntee',\n 'HWmusictweets',\n 'Hamilton',\n 'Hayden_',\n 'HollyCairns',\n 'HomeGuys',\n 'IAMJHUD',\n 'IAU_org',\n 'ID',\n 'ILoveBlue',\n 'IN',\n 'IND',\n 'IRELAND',\n 'IT',\n 'IWF',\n 'IWV',\n 'IYCHimachal',\n 'I_AM_DIANNA',\n 'ImDukeDennis',\n 'InNath',\n 'Inc',\n 'India',\n 'IndiaFire',\n 'Indigenous',\n 'IndoEnts',\n 'IndoSport',\n 'Indo_Travel_',\n 'Indps',\n 'Insider',\n 'Institute',\n 'IoT',\n 'Ireland',\n 'J15',\n 'JDean',\n 'J_M_',\n 'Jake',\n 'Jam',\n 'Jamie_',\n 'Jason33',\n 'John',\n 'John_',\n 'JoshuaMusic',\n 'Jumaane',\n 'JxnPress',\n 'KaggyFilms',\n 'Kateme',\n 'KingCounty',\n 'Klompas',\n 'Kpop',\n 'LFPress',\n 'LGSouthAfrica',\n 'LGUS',\n 'LanceHoyt',\n 'LauraleeB4real',\n 'LayingUp',\n 'Layton',\n 'Learn',\n 'Legion',\n 'Leick',\n 'Lens',\n 'LibyaFZC',\n 'LisaDNews',\n 'Live',\n 'Lotzia',\n 'Louis',\n 'MAS',\n 'MEA',\n 'MNPDNashville',\n 'MSEdgeDev',\n 'MSEducationUK',\n 'MSFTMechanics',\n 'MSIntune',\n 'Maister_SSB',\n 'MajorCA',\n 'Mandy02',\n 'Manitoba',\n 'Michael',\n 'MickryTD',\n 'MidEast',\n 'Mike_P_',\n 'MitsubishiHVAC',\n 'Mobile',\n 'MobileKE',\n 'MobileSA',\n 'MobileUS',\n 'Mobility',\n 'Money',\n 'MontanasBBQ',\n 'Montreal',\n 'MortalGame',\n 'MotilalOswal',\n 'MotorPH',\n 'Movies',\n 'MrCamW',\n 'Mumbai01',\n 'Music',\n 'MusicIN',\n 'My_',\n 'NB',\n 'NEJacks',\n 'NG',\n 'NGR',\n 'NL',\n 'NS',\n 'NYSBA',\n 'NYT',\n 'NatGeo',\n 'NatGeoTV',\n 'Network',\n 'Networking',\n 'News',\n 'News_IN',\n 'Newton',\n 'NextGen',\n 'Office',\n 'OfficialDMRC',\n 'Oklahoma',\n 'Old',\n 'Olympics',\n 'OnTheCoast',\n 'OncMed',\n 'OnyxShow',\n 'OriginalFresca',\n 'Originals',\n 'Oritse',\n 'Ottawa',\n 'OwsWills',\n 'PEI',\n 'PH',\n 'PNB1',\n 'ParamountAU',\n 'ParamountPics',\n 'ParkSF',\n 'Partners',\n 'PatShow',\n 'PaulLAB',\n 'People',\n 'Phil',\n 'Photos',\n 'Pluckers',\n 'Policy',\n 'Politics',\n 'Porsha4real',\n 'Portugal',\n 'PresentsYT',\n 'RFERL',\n 'RJinVegas',\n 'ROJaipur',\n 'Racing',\n 'RadioAsia',\n 'RadioNews',\n 'RadioSkaro',\n 'RailENG',\n 'RailKochi',\n 'RailNagpur',\n 'Refaeli',\n 'RepNC',\n 'Rescue',\n 'Research',\n 'Rob71',\n 'RobTheHockeyGuy',\n 'Rotary',\n 'RoyalNHS',\n 'Rtreat',\n 'Ryzen',\n 'SA',\n 'SD',\n 'SFTHQ',\n 'SNCaroline',\n 'SP360',\n 'Sage',\n 'SanjayWills',\n 'Saudia',\n 'Schools',\n 'ScienceEnv',\n 'ScooterMagruder',\n 'Secure',\n 'Security',\n 'Series',\n 'SethW',\n 'Shane11',\n 'SharePoint',\n 'Showcase',\n 'SimonHarris',\n 'SmallBiz',\n 'SmallBizLady',\n 'SoCal',\n 'Software',\n 'SonnyB',\n 'SouthAfrica',\n 'SouthJersey',\n 'SouthPIO',\n 'SouthPenn',\n 'Space',\n 'SpiderManMovie',\n 'Sports',\n 'SportsCFB',\n 'SportsCtr',\n 'StadiumTX',\n 'Store',\n 'Stores',\n 'Studios',\n 'TGR_WEC',\n 'TK_MiddleEast',\n 'TRVLRSseries',\n 'TV',\n 'TalosSecurity',\n 'Taylor',\n 'Teams',\n 'Tech',\n 'TeoachBuzz',\n 'Terreros',\n 'TheCars',\n 'TheChions',\n 'TheHall',\n 'TheLead',\n 'TheNational',\n 'TheRekhaSha',\n 'TheTradeInn',\n 'Theatres',\n 'Theie',\n 'Tickets',\n 'ToDo',\n 'Toronto',\n 'TourofBritain',\n 'TransitPolice',\n 'Tucky',\n 'TylerJ3',\n 'UAE',\n 'UK',\n 'UKI',\n 'UKNews',\n 'UKTrade',\n 'UK_Life',\n 'US',\n 'USA',\n 'UTCoachSam',\n 'Umbrella',\n 'UnitedPilots',\n 'VMSportIE',\n 'Venuses',\n 'VideoGame',\n 'VisualStudio',\n 'WHIAANI',\n 'WalkingDead_',\n 'WeAre',\n 'Wolfe',\n 'World',\n '_Aus',\n '_BE',\n '_Canada',\n '_Energy',\n '_Ents',\n '_Fortuner',\n '_Gray',\n '_Help',\n '_Home',\n '_IN',\n '_Indonesia',\n '_Ireland',\n '_JAPAN',\n '_Justin',\n '_KE',\n '_Labs',\n '_LetsTalk',\n '_Marcus',\n '_NG',\n '_News',\n '_ROG_IN',\n '_SA',\n '_Sport',\n '_Staff',\n '_TV',\n '_Travel',\n '_UAE',\n '_UK',\n '_US',\n '_Ug',\n '__ang',\n '__lilbooman4',\n '_equal',\n '_europe',\n '_in',\n '_kurumsal',\n '_news',\n '_paige',\n '_saiprasad',\n '_tiff',\n '_tv',\n '_uk',\n 'abedp',\n 'advocates',\n 'aj23',\n 'alankellylabour',\n 'amfam',\n 'andbench',\n 'apac',\n 'arielle',\n 'ash1',\n 'ashleynicwill',\n 'belgie',\n 'bellas',\n 'bing',\n 'books',\n 'brk',\n 'cKSL',\n 'cam_rrie',\n 'camjordan94',\n 'cammyorr95',\n 'camsmith9',\n 'candacecbure',\n 'casilva',\n 'cathmartingreen',\n 'championship',\n 'channelph',\n 'charts',\n 'chasep',\n 'chomicide',\n 'chriswnews',\n 'cloud',\n 'coachwalt',\n 'crystal',\n 'dallas',\n 'deathbomc',\n 'demedellin',\n 'devs',\n 'dj45',\n 'eclarridge',\n 'emily',\n 'engage',\n 'eric_d_',\n 'ericpalicki',\n 'esposito',\n 'film',\n 'flyethiopian',\n 'fmtoday',\n 'football',\n 'fpjindia',\n 'fratmove',\n 'funnybrad',\n 'ggez',\n 'ginos',\n 'globalmedia',\n 'gseattle',\n 'hkfp',\n 'hmrgov',\n 'hoosier',\n 'hunter',\n 'hypatiadotca',\n 'i',\n 'ihristynw',\n 'imdavisss',\n 'imthesmash',\n 'india',\n 'industry',\n 'ireland',\n 'jessedee',\n 'jumper',\n 'kyoag',\n 'labsindia',\n 'ljoy',\n 'loreillysf',\n 'losangeles',\n 'luchalibre',\n 'lucieboots',\n 'magalona',\n 'mark7ft4',\n 'marketplace',\n 'martyn_',\n 'mavis',\n 'middleeast',\n 'mikedj',\n 'mmprideorg',\n 'mo',\n 'mobileIN',\n 'morning',\n 'msPartner',\n 'msdev',\n 'msexcel',\n 'music',\n 'nailogical',\n 'namebrands',\n 'neasa_neasa',\n 'news',\n 'newsbc',\n 'newsroom',\n 'newsteam',\n 'nycpa',\n 'offcl',\n 'official',\n 'onlymamba',\n 'otttraffic',\n 'ow',\n 'p',\n 'ph',\n 'philippines',\n 'playmaker',\n 'podcasts',\n 'poetlogers',\n 'poorna',\n 'primevideosport',\n 'qatar',\n 'qrsupport',\n 'quintonj',\n 'racing',\n 'radio',\n 'railwaykol',\n 'realtim',\n 'rnadalacademy',\n 'roadwatch',\n 'robbie',\n 'rose__',\n 'ryan',\n 'ryan_a_',\n 'saks',\n 'saltpublishing',\n 'sandela',\n 'scheetz',\n 'sennydreadful',\n 'serena',\n 'servers',\n 'sian100',\n 'skigraham',\n 'smoke',\n 'smytho',\n 'speechtv',\n 'sport',\n 'sports',\n 'stadium',\n 'stool_LTB',\n 'surface',\n 'teaxtarot',\n 'tech',\n 'telegraph',\n 'theband',\n 'thecouncil',\n 'thecreativeindp',\n 'thecuffe',\n 'thefront7',\n 'thepaul',\n 'thepedimom',\n 'therealfstl1992',\n 'thisis',\n 'thisisWurlD',\n 'tire',\n 'tom1974',\n 'tomwfootball',\n 'tvUK',\n 'tyler',\n 'ug',\n 'uk',\n 'united',\n 'usa',\n 'vandernet',\n 'w_terrence',\n 'war',\n 'weareradio',\n 'what_eats_owls',\n 'wkamau',\n 'youngand',\n 'zach'}"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nimport networkx as nx\nfrom node2vec import Node2Vec\nimport re\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nimport emoji\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load specialized Twitter model\nmodel_name = \"cardiffnlp/twitter-roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name).to(device)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-10-12T15:27:49.700679Z","iopub.execute_input":"2024-10-12T15:27:49.701014Z","iopub.status.idle":"2024-10-12T15:28:08.793158Z","shell.execute_reply.started":"2024-10-12T15:27:49.700969Z","shell.execute_reply":"2024-10-12T15:28:08.792298Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/565 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c79cd204e7c4db8b7ff2416ad7270a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0745654d639460e8b113dcae51012ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ac8fa9b31934b958074ff3a7758f70a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74dd1584e4824ec089db4ee96f04ea95"}},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"def preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    # Remove user mentions\n    text = re.sub(r'@\\w+', '', text)\n    # Remove non-letter characters\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n    return text\n\ndef get_roberta_embedding(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\ndef get_roberta_embedding_batch(texts):\n    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n\ndef get_datetime_features(date_str):\n    date = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n    return np.array([\n        date.year, date.month, date.day, date.hour, date.minute,\n        date.weekday(), date.timetuple().tm_yday,\n        np.sin(2 * np.pi * date.hour / 24),  # Cyclical encoding for hour\n        np.cos(2 * np.pi * date.hour / 24),\n        np.sin(2 * np.pi * date.month / 12),  # Cyclical encoding for month\n        np.cos(2 * np.pi * date.month / 12)\n    ])\n\ndef get_categorical_embedding(value, encoder):\n    return encoder.transform([[value]]).flatten()\n\ndef create_graph_embedding(df):\n    G = nx.Graph()\n    for _, row in df.iterrows():\n        G.add_edge(row['username'], row['inferred company'])\n        for mention in re.findall(r'@(\\w+)', row['content']):\n            G.add_edge(row['username'], mention)\n    \n    node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n    \n    def get_node_embedding(node):\n        try:\n            return model.wv[node]\n        except KeyError:\n            return np.zeros(64)\n    \n    df['user_graph_embedding'] = df['username'].apply(get_node_embedding)\n    df['company_graph_embedding'] = df['inferred company'].apply(get_node_embedding)\n    return df\n\ndef extract_hashtags(text):\n    return ' '.join(re.findall(r'#(\\w+)', text))\n\ndef extract_emoji(text):\n    return ' '.join([c for c in text if emoji.is_emoji(c)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:28:08.794871Z","iopub.execute_input":"2024-10-12T15:28:08.795840Z","iopub.status.idle":"2024-10-12T15:28:08.815908Z","shell.execute_reply.started":"2024-10-12T15:28:08.795789Z","shell.execute_reply":"2024-10-12T15:28:08.814707Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# # Label encode the binned categories\n# le = LabelEncoder()\n# y = le.fit_transform(train_data_sampled['likes_binned'])\n\n# # Define the groups based on the 'inferred_company'\n# groups = train_data_sampled['inferred company']\n\n# # Initialize GroupShuffleSplit with stratification\n# gss = StratifiedGroupKFold(n_splits=5,shuffle=True, random_state=42)\n# train_idx, val_idx = next(gss.split(train_data_sampled, train_data_sampled['likes_binned'], groups))\n# # Perform the split\n# # for train_idx, val_idx in gss.split(X_combined, y, groups):\n# X_train, X_val = train_data_sampled.iloc[train_idx].reset_index(drop=True), train_data_sampled.iloc[val_idx].reset_index(drop=True)\n# y_train, y_val = train_data_sampled['likes_capped'].iloc[train_idx].reset_index(drop=True), train_data_sampled['likes_capped'].iloc[val_idx].reset_index(drop=True)\n\n# # Ensure stratification within the group-based split\n# # print(f\"Training set class distribution: {np.bincount(y_train)}\")\n# # print(f\"Validation set class distribution: {np.bincount(y_val)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:28:08.817366Z","iopub.execute_input":"2024-10-12T15:28:08.817734Z","iopub.status.idle":"2024-10-12T15:28:08.833965Z","shell.execute_reply.started":"2024-10-12T15:28:08.817701Z","shell.execute_reply":"2024-10-12T15:28:08.833000Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Load the data\ndf = train_data_sampled.copy()\n\n# Preprocess text and combine data\nprint(\"Preprocessing train text...\")\ndf['processed_content'] = df['content'].apply(preprocess_text)\ndf['hashtags'] = df['content'].apply(extract_hashtags)\ndf['emojis'] = df['content'].apply(extract_emoji)\ndf['combined_text'] = df.apply(lambda row: f\"tweet related to: {row['username']} is {row['processed_content']} {row['hashtags']} {row['emojis']}\", axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:28:08.835165Z","iopub.execute_input":"2024-10-12T15:28:08.835535Z","iopub.status.idle":"2024-10-12T15:28:09.696369Z","shell.execute_reply.started":"2024-10-12T15:28:08.835498Z","shell.execute_reply":"2024-10-12T15:28:09.695300Z"}},"outputs":[{"name":"stdout","text":"Preprocessing train text...\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nbatch_size = 64\n\n# Create DataLoader for the text data\ndataloader = DataLoader(df['combined_text'].tolist(), batch_size=batch_size, shuffle=False)\n\nprint(\"Generating embeddings...\")\nembeddings = []\n\n# Loop through the batches and generate embeddings\nfor batch in tqdm(dataloader, desc=\"Generating text embeddings\"):\n    batch_embeddings = get_roberta_embedding_batch(batch)\n    embeddings.extend(batch_embeddings)\n\n# Add the embeddings to the dataframe\ndf['combined_embedding'] = embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:28:09.697604Z","iopub.execute_input":"2024-10-12T15:28:09.697931Z"}},"outputs":[{"name":"stdout","text":"Generating embeddings...\n","output_type":"stream"},{"name":"stderr","text":"Generating text embeddings:  36%|‚ñà‚ñà‚ñà‚ñå      | 17/47 [00:02<00:04,  7.13it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Generate other embeddings\nprint(\"Generating other embeddings...\")\ndf['datetime_embedding'] = df['date'].apply(get_datetime_features)\n\n# company_encoder = OneHotEncoder(sparse=False)\n# company_encoder.fit(df['inferred company'].values.reshape(-1, 1))\n# df['company_embedding'] = df['inferred company'].apply(lambda x: get_categorical_embedding(x, company_encoder))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom transformers import pipeline, AutoTokenizer, AutoModel\nfrom sklearn.decomposition import PCA\nimport torch\nfrom tqdm import tqdm  # For progress tracking\n\n# Load the training data\n# df = pd.read_csv(\"train_tweets.csv\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Check for GPU\n\n# 1. Tokenize usernames and perform word frequency analysis\ndef tokenize_username(username):\n    username = username.replace('_', ' ')\n    username_parts = re.findall(r'[A-Z][a-z]*|[a-z]+|\\d+', username)\n    return [word.lower() for word in username_parts]\n\ndf['tokenized_username'] = df['username'].apply(tokenize_username)\n\n# 5. Username Embedding Generation using Pre-trained Model\nmodel_name = \"cardiffnlp/twitter-roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name).to(device)  # Move model to GPU\n\ndef get_username_embedding_batch(usernames):\n    tokenized = tokenizer(usernames, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n    with torch.no_grad():\n        outputs = model(**tokenized)\n    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n\n# Batch size for processing\nbatch_size = 64\n\n# Tokenize usernames and store them in a list\ntokenized_usernames = [' '.join(tokenize_username(username)) for username in df['username'].tolist()]\n\n# Create DataLoader for the usernames\ndataloader = DataLoader(tokenized_usernames, batch_size=batch_size, shuffle=False)\n\n# Generate embeddings in batches\nprint(\"Generating username embeddings...\")\nembeddings = []\n\nfor batch in tqdm(dataloader, desc=\"Embedding Generation\"):\n    batch_embeddings = get_username_embedding_batch(batch)\n    embeddings.extend(batch_embeddings)\n\n# Add embeddings back to the DataFrame\ndf['username_embedding'] = embeddings\n# 6. Dimensionality Reduction with PCA\npca = PCA(n_components=64)\ndf['reduced_username_embedding'] = list(pca.fit_transform(np.stack(df['username_embedding'])))\n\n# 7. Feature List\nprint(\"Training features generated successfully!\")\nprint(\"Feature columns for use in neural network:\")\nprint(df.columns)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine all embeddings\nprint(\"Combining embeddings...\")\ndf['rich_embedding'] = df.apply(lambda row: np.concatenate([\n    row['combined_embedding'],\n    row['datetime_embedding'],\n    row['reduced_username_embedding']\n    # row['company_embedding']\n    # row['user_graph_embedding'],\n    # row['company_graph_embedding']\n]), axis=1)\n\nprint(f\"Shape of rich embedding: {df['rich_embedding'].iloc[0].shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pca.explained_variance_ratio_.sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Load the data\n# df = X_val\n\n# # Preprocess text and combine data\n# print(\"Preprocessing val text...\")\n# df['processed_content'] = df['content'].apply(preprocess_text)\n# df['hashtags'] = df['content'].apply(extract_hashtags)\n# df['emojis'] = df['content'].apply(extract_emoji)\n# df['combined_text'] = df.apply(lambda row: f\"{row['processed_content']} {row['username']} {row['inferred company']} {row['hashtags']} {row['emojis']}\", axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Generate embeddings with progress bar\n# print(\"Generating embeddings...\")\n# embeddings = []\n# for text in tqdm(df['combined_text'], desc=\"Generating text embeddings\"):\n#     embeddings.append(get_roberta_embedding(text))\n# df['combined_embedding'] = embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Generate other embeddings\n# print(\"Generating other embeddings...\")\n# df['datetime_embedding'] = df['date'].apply(get_datetime_features)\n\n# # company_encoder = OneHotEncoder(sparse=False)\n# # company_encoder.fit(df['inferred company'].values.reshape(-1, 1))\n# df['company_embedding'] = df['inferred company'].apply(lambda x: get_categorical_embedding(x, company_encoder))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"Creating graph embeddings...\")\n# df = create_graph_embedding(df)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n\n\n# Prepare data for regression\n# X = np.stack(df['rich_embedding'])\ny = df['likes_capped'].values\n\n# Log transform the target variable to handle skewness\ny = np.log1p(y)  # log1p is used to handle zero values\n\n# Split the data\n# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom bertopic import BERTopic\nimport preprocessor as p\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom textblob import TextBlob\n\n# Download necessary NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('words')\n\n# Load English words for username cleaning\nenglish_words = set(nltk.corpus.words.words())\n\n# Preprocess tweets\ndef preprocess_tweet(tweet):\n    # Count mentions and hyperlinks\n    mention_count = tweet.count('<mention>')\n    hyperlink_count = tweet.count('<hyperlink>')\n    \n    # Remove mentions and hyperlinks\n    tweet = tweet.replace('<mention>', '').replace('<hyperlink>', '')\n    \n    # Clean the tweet\n    tweet = p.clean(tweet)\n    # Tokenize\n    tokens = word_tokenize(tweet.lower())\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token not in stop_words]\n    \n    return ' '.join(tokens), mention_count, hyperlink_count\n\n# Clean username\ndef clean_username(username):\n    # Remove special characters and numbers\n    clean_name = re.sub(r'[^a-zA-Z]', ' ', username)\n    # Split into parts\n    parts = clean_name.split()\n    # Keep only parts that are likely to be meaningful words\n    meaningful_parts = [part.lower() for part in parts if len(part) > 2 and (part.lower() in english_words or not part.isalpha())]\n    return ' '.join(meaningful_parts)\n\n# Load your data (assuming you have a CSV with 'content' and 'username' columns)\ndf = train_data_sampled.copy()\n\n# Preprocess tweets and usernames\ndf['processed_content'], df['mention_count'], df['hyperlink_count'] = zip(*df['content'].apply(preprocess_tweet))\ndf['processed_username'] = df['username'].apply(clean_username)\n\n# Combine processed content and processed username\ndf['text'] = df['processed_content'] + ' ' + df['processed_username']\ndef get_sentiment(text):\n    return TextBlob(text).sentiment.polarity\n\ndf['sentiment'] = df['text'].apply(get_sentiment)\n# Split the data first to prevent data leakage\ntrain_df, val_df, y_train, y_val = train_test_split(df,y, test_size=0.2, random_state=42)\ntrain_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n\n# Perform topic modeling on the training set\ndef perform_topic_modeling(train_texts, val_texts, n_topics=10):\n    # LDA\n    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n    train_doc_term_matrix = vectorizer.fit_transform(train_texts)\n    val_doc_term_matrix = vectorizer.transform(val_texts)\n    \n    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n    train_lda_output = lda.fit_transform(train_doc_term_matrix)\n    val_lda_output = lda.transform(val_doc_term_matrix)\n    \n    # BERTopic\n    topic_model = BERTopic(nr_topics=n_topics)\n    train_topics, _ = topic_model.fit_transform(train_texts)\n    val_topics, _ = topic_model.transform(val_texts)\n    \n    return train_lda_output, val_lda_output, train_topics, val_topics\n\ntrain_lda, val_lda, train_bertopics, val_bertopics = perform_topic_modeling(train_df['content'], val_df['content'])\n\n# Add topic columns to the dataframes\nfor i in range(train_lda.shape[1]):\n    train_df[f'lda_topic_{i}'] = train_lda[:, i]\n    val_df[f'lda_topic_{i}'] = val_lda[:, i]\n\ntrain_df['bertopic'] = train_bertopics\nval_df['bertopic'] = val_bertopics\nfeature_columns = [col for col in train_df.columns if col.startswith('lda_topic_')] + ['mention_count', 'hyperlink_count', 'sentiment', 'bertopic']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T16:11:44.748949Z","iopub.execute_input":"2024-10-12T16:11:44.750158Z","iopub.status.idle":"2024-10-12T16:12:22.589351Z","shell.execute_reply.started":"2024-10-12T16:11:44.750099Z","shell.execute_reply":"2024-10-12T16:12:22.588036Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package words to /usr/share/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"# Combine all embeddings\nprint(\"Combining embeddings...\")\ntrain_df['rich_embedding'] = train_df.apply(lambda row: np.concatenate([\n    row['combined_embedding'],\n    row['datetime_embedding'],\n    \n    # row['company_embedding']\n    # row['user_graph_embedding'],\n    # row['company_graph_embedding']\n]), axis=1)\ntrain_df['other_embeddings'] = train_df.apply(lambda row: np.concatenate([\n    row[cols] for cols in feature_columns\n]), axis=1)\ntrain_df['final_embedding'] = train_df.apply(lambda row: np.concatenate([\n    row['rich_embedding'],\n    row['other_embeddings'],\n    \n    # row['company_embedding']\n    # row['user_graph_embedding'],\n    # row['company_graph_embedding']\n]), axis=1)\nprint(f\"Shape of final embedding: {train_df['final_embedding'].iloc[0].shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine all embeddings\nprint(\"Combining embeddings...\")\nval_df['rich_embedding'] = val_df.apply(lambda row: np.concatenate([\n    row['combined_embedding'],\n    row['datetime_embedding'],\n    \n    # row['company_embedding']\n    # row['user_graph_embedding'],\n    # row['company_graph_embedding']\n]), axis=1)\nval_df['other_embeddings'] = val_df.apply(lambda row: np.concatenate([\n    row[cols] for cols in feature_columns\n]), axis=1)\nval_df['final_embedding'] = val_df.apply(lambda row: np.concatenate([\n    row['rich_embedding'],\n    row['other_embeddings'],\n    \n    # row['company_embedding']\n    # row['user_graph_embedding'],\n    # row['company_graph_embedding']\n]), axis=1)\nprint(f\"Shape of final embedding: {val_df['final_embedding'].iloc[0].shape}\")val_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_train = np.stack(train_df['rich_embedding'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a neural network for regression\nclass TweetLikesRegressor(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(TweetLikesRegressor, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc3 = nn.Linear(hidden_dim // 2, 1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.3)\n    \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x.squeeze()\n\n# Create dataset and dataloader\nclass TweetDataset(Dataset):\n    def __init__(self, embeddings, likes):\n        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n        self.likes = torch.tensor(likes, dtype=torch.float32)\n    \n    def __len__(self):\n        return len(self.likes)\n    \n    def __getitem__(self, idx):\n        return self.embeddings[idx], self.likes[idx]\n\n# Hyperparameters\ninput_dim = X_train.shape[1]\nhidden_dim = 256\nbatch_size = 64\nnum_epochs = 100\nlearning_rate = 0.001","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create model, optimizer, and loss function\nmodel = TweetLikesRegressor(input_dim, hidden_dim).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.MSELoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create datasets and dataloaders\ntrain_dataset = TweetDataset(X_train_scaled, y_train)\nval_dataset = TweetDataset(X_val_scaled, y_val)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nprint(\"Training regression model...\")\nbest_val_mse = float('inf')\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for batch_embeddings, batch_likes in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        batch_embeddings, batch_likes = batch_embeddings.to(device), batch_likes.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_embeddings)\n        loss = criterion(outputs, batch_likes)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    # Validation\n    model.eval()\n    val_preds = []\n    val_true = []\n    with torch.no_grad():\n        for batch_embeddings, batch_likes in val_loader:\n            batch_embeddings, batch_likes = batch_embeddings.to(device), batch_likes.to(device)\n            outputs = model(batch_embeddings)\n            val_preds.extend(outputs.cpu().numpy())\n            val_true.extend(batch_likes.cpu().numpy())\n    \n    val_mse = mean_squared_error(val_true, val_preds)\n    val_mae = mean_absolute_error(val_true, val_preds)\n    val_r2 = r2_score(val_true, val_preds)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n    print(f\"Val MSE: {val_mse:.4f}, Val MAE: {val_mae:.4f}, Val R2: {val_r2:.4f}\")\n    \n    # Save the best model\n    if val_mse < best_val_mse:\n        best_val_mse = val_mse\n        torch.save(model.state_dict(), 'best_tweet_likes_regressor.pth')\n        print(\"New best model saved!\")\n\nprint(\"Training completed!\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to get predictions for new data\ndef predict_likes(new_data):\n    model.eval()\n    with torch.no_grad():\n        new_data_scaled = scaler.transform(new_data)\n        embeddings = torch.tensor(new_data_scaled, dtype=torch.float32).to(device)\n        outputs = model(embeddings)\n        # Convert log predictions back to original scale\n        return np.expm1(outputs.cpu().numpy())\n\n# Example usage:\n# new_likes_predictions = predict_likes(new_embeddings)\nmodel.load_state_dict(torch.load('best_tweet_likes_regressor.pth'))\n# Evaluate on test set\nmodel.eval()\ntest_preds = []\ntest_true = y_val  # Using validation set as test set for this example\nwith torch.no_grad():\n    for batch_embeddings, batch_likes in val_loader:\n        batch_embeddings = batch_embeddings.to(device)\n        outputs = model(batch_embeddings)\n        test_preds.extend(outputs.cpu().numpy())\n\n# Convert predictions back to original scale\ntest_preds = np.expm1(test_preds)\ntest_true = np.expm1(test_true)\n\n# Calculate metrics on original scale\ntest_mse = mean_squared_error(test_true, test_preds)\ntest_mae = mean_absolute_error(test_true, test_preds)\ntest_r2 = r2_score(test_true, test_preds)\n\nprint(\"\\nTest Set Evaluation (Original Scale):\")\nprint(f\"MSE: {test_mse:.2f}\")\nprint(f\"MAE: {test_mae:.2f}\")\nprint(f\"R2: {test_r2:.4f}\")\n\n# Calculate custom metrics for skewed data\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef median_absolute_percentage_error(y_true, y_pred):\n    return np.median(np.abs((y_true - y_pred) / y_true)) * 100\n\nmape = mean_absolute_percentage_error(test_true, test_preds)\nmdape = median_absolute_percentage_error(test_true, test_preds)\n\nprint(f\"MAPE: {mape:.2f}%\")\nprint(f\"MdAPE: {mdape:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"temp_df = pd.DataFrame(test_preds)\ntemp_df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"temp_df = pd.DataFrame(test_true)\ntemp_df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate Pearson correlation coefficient\ncorrelation = np.corrcoef(test_preds, test_true)[0,1]\n\nprint(f\"Pearson correlation: {correlation}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.scatter(test_preds,test_true)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare data for classification\nX = np.stack(df['rich_embedding'])\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df['likes_binned'])  # Assuming 'y' column exists with class labels\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a simple neural network for classification\nclass TweetClassifier(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super(TweetClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# Create dataset and dataloader\nclass TweetDataset(Dataset):\n    def __init__(self, embeddings, labels):\n        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.embeddings[idx], self.labels[idx]\n\n# Hyperparameters\ninput_dim = X_train.shape[1]\nhidden_dim = 256\nnum_classes = len(np.unique(y))\nbatch_size = 64\nnum_epochs = 100\nlearning_rate = 0.001","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create model, optimizer, and loss function\nmodel = TweetClassifier(input_dim, hidden_dim, num_classes).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create datasets and dataloaders\ntrain_dataset = TweetDataset(X_train, y_train)\nval_dataset = TweetDataset(X_val, y_val)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nprint(\"Training classification model...\")\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for batch_embeddings, batch_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        batch_embeddings, batch_labels = batch_embeddings.to(device), batch_labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(batch_embeddings)\n        loss = criterion(outputs, batch_labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    # Validation\n    model.eval()\n    val_preds = []\n    val_true = []\n    with torch.no_grad():\n        for batch_embeddings, batch_labels in val_loader:\n            batch_embeddings, batch_labels = batch_embeddings.to(device), batch_labels.to(device)\n            outputs = model(batch_embeddings)\n            _, preds = torch.max(outputs, 1)\n            val_preds.extend(preds.cpu().numpy())\n            val_true.extend(batch_labels.cpu().numpy())\n    \n    val_accuracy = accuracy_score(val_true, val_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(val_true, val_preds, average='weighted')\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n\nprint(\"Training completed!\")\n\n# Save the model\ntorch.save(model.state_dict(), 'tweet_classifier.pth')\nprint(\"Model saved successfully!\")\n\n# Function to get predictions for new data\ndef predict(new_data):\n    model.eval()\n    with torch.no_grad():\n        embeddings = torch.tensor(new_data, dtype=torch.float32).to(device)\n        outputs = model(embeddings)\n        _, preds = torch.max(outputs, 1)\n    return label_encoder.inverse_transform(preds.cpu().numpy())\n\n# Example usage:\n# new_predictions = predict(new_embeddings)","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}