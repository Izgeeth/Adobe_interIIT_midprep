{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9548807,"sourceType":"datasetVersion","datasetId":5817858},{"sourceId":9555976,"sourceType":"datasetVersion","datasetId":5822842}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pickle\nimport nest_asyncio\nimport asyncio\nimport aiohttp\nimport aiofiles\nimport io\nimport sys\nimport psutil\nimport re\nimport math\nimport gc\nimport os\nfrom PIL import Image\nfrom io import BytesIO\nfrom tqdm.notebook import tqdm_notebook as tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport requests\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom transformers import pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['TOKENIZERS_PARALLELISM'] = 'true'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_to_text = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\", device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/adobetraindata/behaviour_simulation_train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quantiles = df['likes'].quantile(np.linspace(0, 1, 8)).values\nlabels = [f'{int(quantiles[i])}-{int(quantiles[i + 1])}' for i in range(len(quantiles) - 1)]\ndf['likes_binned'] = pd.cut(df['likes'], bins=quantiles, labels=labels, include_lowest=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_fraction = 1/30  \ndf, _ = train_test_split(df, train_size=sample_fraction, random_state=42, stratify=df['likes_binned'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['likes_binned'].value_counts(normalize=True).sort_index())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_link(media):\n    pattern = r\"(?:Photo\\(previewUrl|Video\\(thumbnailUrl|Gif\\(thumbnailUrl)='([^']*)'\"\n    match = re.search(pattern, media)\n    if match:\n        return match.group(1)\n    return None\n\ndf['link'] = df['media'].apply(extract_link)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"async def fetch_image(session, url):\n    async with session.get(url) as response:\n        if response.status == 200:\n            img_data = await response.read()\n            return img_data\n\nasync def download_images(urls):\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_image(session, url) for url in urls]\n        return await asyncio.gather(*tasks)\n\ndf['image'] = await download_images(list(df['link']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.reset_index(drop=True)\ndf.shape\ndf_valid = df[df['image'].notna()].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureDataset(nn.Module):\n    def __init__(self, df):\n        self.df = df\n    \n    def __len__(self):\n        return(self.df.shape[0])\n    \n    def __getitem__(self, index):\n        return {\n            'id' : int(self.df['id'][index]),\n            'img': Image.open(io.BytesIO(self.df['image'][index])).convert('RGB')\n            }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_li_data = FeatureDataset(df_valid[['id', 'image']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    ids = [item['id'] for item in batch]\n    images = [item['img'] for item in batch]\n    \n    return {\n        'id': ids,\n        'img': images\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_li_load = DataLoader(temp_li_data, batch_size = 128, shuffle = False, num_workers = 4, collate_fn=collate_fn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"li = []\n\nfor data in tqdm(temp_li_load):\n    torch.cuda.empty_cache()\n    ids = data['id']\n    images = data['img']\n\n    with torch.no_grad():\n        generated_text = image_to_text(images)\n          \n    li.extend((t, text) for t, text in zip(ids, generated_text)) \n    \n    # Clear memory\n    del images, ids, generated_text\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}